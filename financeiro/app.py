import streamlit as st
from PyPDF2 import PdfReader
from openai import OpenAI
from typing import List
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ==============================
# ğŸ§­ ConfiguraÃ§Ã£o da PÃ¡gina
# ==============================
st.set_page_config(
    page_title="Busca Inteligente em PDFs - Mercado Financeiro",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ==============================
# ğŸ§  CabeÃ§alho
# ==============================
st.title("ğŸ“Š Sistema de Busca Inteligente para PDFs")
st.markdown("### AnÃ¡lise de documentos sobre Mercado Financeiro com IA (GPT-4)")

# ==============================
# âš™ï¸ Sidebar
# ==============================
with st.sidebar:
    st.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password", help="Insira sua chave da OpenAI")

    st.markdown("---")
    st.header("ğŸ“ Upload de PDFs")
    uploaded_files = st.file_uploader(
        "Carregue seus arquivos PDF",
        type=['pdf'],
        accept_multiple_files=True
    )

    st.markdown("---")
    st.markdown("""
    ### ğŸ’¡ Exemplos de perguntas:
    - Qual foi o impacto da taxa Selic?
    - Como funciona a alocaÃ§Ã£o de ativos?
    - Quais sÃ£o as tendÃªncias do mercado?
    """)

# ==============================
# ğŸ“„ FunÃ§Ãµes auxiliares
# ==============================

def extract_text_from_pdfs(pdf_files):
    """Extrai texto de mÃºltiplos PDFs."""
    text = ""
    for pdf_file in pdf_files:
        try:
            pdf_reader = PdfReader(pdf_file)
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        except Exception as e:
            st.error(f"Erro ao ler {pdf_file.name}: {str(e)}")
    return text


def split_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Divide o texto em chunks menores."""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap
    
    return chunks


def get_embedding(text: str, client) -> List[float]:
    """ObtÃ©m embedding usando OpenAI."""
    try:
        response = client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {str(e)}")
        return []


def find_relevant_chunks(query: str, chunks: List[str], embeddings: List, client, top_k: int = 3) -> List[str]:
    """Encontra os chunks mais relevantes para a pergunta."""
    query_embedding = get_embedding(query, client)
    
    if not query_embedding:
        return chunks[:top_k]
    
    # Calcular similaridade
    similarities = cosine_similarity([query_embedding], embeddings)[0]
    
    # Pegar os top_k mais similares
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [chunks[i] for i in top_indices]


def generate_answer(question: str, context: str, chat_history: List, client) -> str:
    """Gera resposta usando GPT-4."""
    
    # Construir histÃ³rico de mensagens
    messages = [
        {"role": "system", "content": "VocÃª Ã© um assistente especializado em mercado financeiro. Responda baseado apenas no contexto fornecido. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga que nÃ£o encontrou."}
    ]
    
    # Adicionar histÃ³rico
    for msg in chat_history[-6:]:  # Ãšltimas 3 interaÃ§Ãµes
        messages.append({"role": msg["role"], "content": msg["content"]})
    
    # Adicionar pergunta atual com contexto
    messages.append({
        "role": "user",
        "content": f"Contexto dos documentos:\n{context}\n\nPergunta: {question}"
    })
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Erro ao gerar resposta: {str(e)}"


# ==============================
# ğŸ” Estado da SessÃ£o
# ==============================
if 'chunks' not in st.session_state:
    st.session_state.chunks = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = []
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'processed' not in st.session_state:
    st.session_state.processed = False

# ==============================
# ğŸ§© Processamento dos PDFs
# ==============================
if uploaded_files and api_key:
    if not st.session_state.processed:
        with st.spinner("ğŸ“– Processando PDFs..."):
            try:
                # Configurar cliente OpenAI - CORREÃ‡ÃƒO AQUI
                client = OpenAI(api_key=api_key)
                
                # Extrair texto
                raw_text = extract_text_from_pdfs(uploaded_files)

                if raw_text.strip():
                    # Dividir em chunks
                    st.session_state.chunks = split_text_into_chunks(raw_text)
                    
                    # Criar embeddings
                    progress_bar = st.progress(0)
                    embeddings = []
                    
                    for i, chunk in enumerate(st.session_state.chunks):
                        embedding = get_embedding(chunk, client)
                        if embedding:
                            embeddings.append(embedding)
                        progress_bar.progress((i + 1) / len(st.session_state.chunks))
                    
                    st.session_state.embeddings = embeddings
                    st.session_state.processed = True
                    st.session_state.client = client
                    
                    st.success(f"âœ… {len(uploaded_files)} arquivo(s) processado(s) com sucesso!")
                    st.info(f"ğŸ“Š {len(st.session_state.chunks)} segmentos de texto criados")
                else:
                    st.error("âŒ NÃ£o foi possÃ­vel extrair texto dos PDFs.")
            except Exception as e:
                st.error(f"Erro ao processar PDFs: {str(e)}")

# ==============================
# ğŸ’¬ Interface de Chat
# ==============================
if st.session_state.processed:
    st.markdown("---")
    st.header("ğŸ’¬ Chat Inteligente")

    # HistÃ³rico
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # Entrada do usuÃ¡rio
    if user_question := st.chat_input("FaÃ§a sua pergunta sobre o conteÃºdo dos PDFs..."):
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.write(user_question)

        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                try:
                    # Encontrar chunks relevantes
                    relevant_chunks = find_relevant_chunks(
                        user_question,
                        st.session_state.chunks,
                        st.session_state.embeddings,
                        st.session_state.client
                    )
                    
                    # Criar contexto
                    context = "\n\n".join(relevant_chunks)
                    
                    # Gerar resposta
                    answer = generate_answer(
                        user_question,
                        context,
                        st.session_state.chat_history,
                        st.session_state.client
                    )
                    
                    st.write(answer)
                    st.session_state.chat_history.append({"role": "assistant", "content": answer})

                    # Mostrar fontes
                    with st.expander("ğŸ“„ Trechos relevantes consultados"):
                        for i, chunk in enumerate(relevant_chunks):
                            st.markdown(f"**Trecho {i+1}:**")
                            st.text(chunk[:400] + "...")
                            st.markdown("---")
                            
                except Exception as e:
                    st.error(f"Erro ao processar pergunta: {str(e)}")
else:
    st.info("ğŸ‘† FaÃ§a upload dos PDFs e insira sua API Key na barra lateral para comeÃ§ar!")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("### ğŸ“š Passo 1\nEnvie seus PDFs sobre mercado financeiro")
    with col2:
        st.markdown("### ğŸ”‘ Passo 2\nInsira sua OpenAI API Key")
    with col3:
        st.markdown("### ğŸ’¬ Passo 3\nFaÃ§a perguntas e obtenha respostas!")

# ==============================
# ğŸ¦¾ RodapÃ©
# ==============================
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>Desenvolvido com â¤ï¸ usando OpenAI e Streamlit</p>
</div>
""", unsafe_allow_html=True)